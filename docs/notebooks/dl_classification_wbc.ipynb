{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bda0d9507411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralNetClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skorch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from iflai.dl.util import read_data, get_statistics_h5, calculate_weights\n",
    "from iflai.dl.dataset import train_validation_test_split_wth_augmentation, Dataset_Generator_Preprocessed_h5\n",
    "from iflai.ml.feature_extractor import AmnisData\n",
    "from iflai.dl.models import PretrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LRScheduler, Checkpoint\n",
    "import torch.optim as optim\n",
    "from skorch.helper import predefined_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"wbc\"\n",
    "only_channels = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "path_to_data =\"../../data/WBC\"\n",
    "scaling_factor = 255.\n",
    "reshape_size = 64\n",
    "num_channels = len(only_channels)\n",
    "train_transform = transforms.Compose(\n",
    "        [transforms.RandomVerticalFlip(),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(45)])\n",
    "test_transform = transforms.Compose([])\n",
    "batch_size = 256\n",
    "num_workers = 2\n",
    "dev=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amnis_data = AmnisData(path_to_data, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, CLASS_NAMES, data_map = read_data(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(data_map.keys())\n",
    "train_indx, validation_indx, test_indx = train_validation_test_split_wth_augmentation(X, y, only_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset_Generator_Preprocessed_h5(path_to_data=path_to_data,\n",
    "                                                      set_indx=train_indx,\n",
    "                                                      scaling_factor=scaling_factor,\n",
    "                                                      reshape_size=reshape_size,\n",
    "                                                      transform=train_transform,\n",
    "                                                      data_map=data_map,\n",
    "                                                      only_channels=only_channels,\n",
    "                                                      num_channels=num_channels)\n",
    "\n",
    "trainloader = DataLoader(train_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = get_statistics_h5(trainloader, only_channels, None, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data_map.get(y[i]) for i in train_indx]\n",
    "weights = calculate_weights(y_train)\n",
    "class_weights = torch.FloatTensor(weights).to(dev)\n",
    "oversample = RandomOverSampler(random_state=seed_value, sampling_strategy='all')\n",
    "train_indx, y_train = oversample.fit_resample(np.asarray(train_indx).reshape(-1, 1), np.asarray(y_train))\n",
    "train_indx = train_indx.T[0]\n",
    "y_train = [data_map.get(y[i]) for i in train_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset_Generator_Preprocessed_h5(path_to_data=path_to_data,\n",
    "                                                      set_indx=train_indx,\n",
    "                                                      scaling_factor=scaling_factor,\n",
    "                                                      reshape_size=reshape_size,\n",
    "                                                      transform=train_transform,\n",
    "                                                      data_map=data_map,\n",
    "                                                      only_channels=only_channels,\n",
    "                                                      num_channels=num_channels,\n",
    "                                                      means=statistics[\"mean\"],\n",
    "                                                      stds=statistics[\"std\"],\n",
    "                                                  return_only_image=True,\n",
    "                                                      )\n",
    "\n",
    "validation_dataset = Dataset_Generator_Preprocessed_h5(path_to_data=path_to_data,\n",
    "                                                           set_indx=validation_indx,\n",
    "                                                           scaling_factor=scaling_factor,\n",
    "                                                           reshape_size=reshape_size,\n",
    "                                                           transform=test_transform,\n",
    "                                                           data_map=data_map,\n",
    "                                                           only_channels=only_channels,\n",
    "                                                           num_channels=num_channels,\n",
    "                                                           means=statistics[\"mean\"],\n",
    "                                                           stds=statistics[\"std\"],\n",
    "                                                       return_only_image=True,\n",
    "                                                           )\n",
    "\n",
    "test_dataset = Dataset_Generator_Preprocessed_h5(path_to_data=path_to_data,\n",
    "                                                     set_indx=test_indx,\n",
    "                                                     scaling_factor=scaling_factor,\n",
    "                                                     reshape_size=reshape_size,\n",
    "                                                     transform=test_transform,\n",
    "                                                     data_map=data_map,\n",
    "                                                     only_channels=only_channels,\n",
    "                                                     num_channels=num_channels,\n",
    "                                                     means=statistics[\"mean\"],\n",
    "                                                     stds=statistics[\"std\"],\n",
    "                                                 return_only_image=True,\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrscheduler = LRScheduler(policy='StepLR', step_size=7, gamma=0.5)\n",
    "checkpoint = Checkpoint(f_params='wbs_net_all.pth', monitor='valid_loss_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    PretrainedModel, \n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    criterion__weight=class_weights,\n",
    "    lr=1e-5,\n",
    "    batch_size=256,\n",
    "    max_epochs=10,\n",
    "    module__output_features=num_classes,\n",
    "    module__num_channels=num_channels, \n",
    "    optimizer=optim.Adam,\n",
    "    iterator_train__shuffle=False,\n",
    "    iterator_train__num_workers=2,\n",
    "    iterator_valid__shuffle=False,\n",
    "    iterator_valid__num_workers=2,\n",
    "    callbacks=[lrscheduler, checkpoint],\n",
    "    train_split=predefined_split(validation_dataset),\n",
    "    device=dev\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_net = net.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred_net, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
