{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from imageio import imread\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from util import read_data_with_data_map, get_statistics_only_mean_std, calculate_weights, train_validation_test_split\n",
    "from dataset import DatasetGeneratorPreprocessedH5\n",
    "from IPython.core.debugger import Tracer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# Compare Algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from models import PretrainedModel, resnet18\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "import torch.optim as optim\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all random seeds to the specific value, so the results are more reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"wbc\"\n",
    "CLASS_NAMES =  [' unknown', ' CD4+ T', ' CD8+ T', ' CD15+ neutrophil', ' CD14+ monocyte', ' CD19+ B', ' CD56+ NK', ' NKT', ' eosinophil']\n",
    "only_channels = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "path_to_data =\"..\\..\\data/WBC/experiment/donor/condition\"\n",
    "model_dir = \"models_remote\"\n",
    "scaling_factor = 4095.\n",
    "reshape_size = 64\n",
    "num_channels = len(only_channels)\n",
    "train_transform = transforms.Compose(\n",
    "        [\n",
    "            #transforms.ToPILImage(),\n",
    "         transforms.RandomVerticalFlip(),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(1),\n",
    "         #transforms.ToTensor()\n",
    "        ])\n",
    "test_transform = transforms.Compose([])\n",
    "channels = np.array([\"BF1\", \"CD15\", \"SigL8\", \"CD14\", \"CD19\", \"DF\", \"CD3\", \"CD45\", \"BF2\", \"CD4\", \"CD56\", \"CD8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 2\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, class_names, data_map = read_data_with_data_map(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(data_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(CLASS_NAMES) == num_classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indx, validation_indx, test_indx = train_validation_test_split(X, y, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetGeneratorPreprocessedH5(path_to_data=path_to_data,\n",
    "                                                      set_indx=train_indx,\n",
    "                                                      scaling_factor=scaling_factor,\n",
    "                                                      reshape_size=reshape_size,\n",
    "                                                      transform=train_transform,\n",
    "                                                      data_map=data_map,\n",
    "                                                      only_channels=only_channels,\n",
    "                                                      num_channels=num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversamle and use class weights for imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data_map.get(y[i]) for i in train_indx]\n",
    "weights = calculate_weights(y_train)\n",
    "oversample = RandomOverSampler(random_state=seed_value, sampling_strategy='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indx, y_train = oversample.fit_resample(np.asarray(train_indx).reshape(-1, 1), np.asarray(y_train))\n",
    "train_indx = train_indx.T[0]\n",
    "y_train = [data_map.get(y[i]) for i in train_indx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate statistics of train set and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = get_statistics_only_mean_std(train_loader, only_channels, None, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetGeneratorPreprocessedH5(path_to_data=path_to_data,\n",
    "                                                      set_indx=train_indx,\n",
    "                                                      scaling_factor=scaling_factor,\n",
    "                                                      reshape_size=reshape_size,\n",
    "                                                      transform=train_transform,\n",
    "                                                      data_map=data_map,\n",
    "                                                      only_channels=only_channels,\n",
    "                                                      num_channels=num_channels,\n",
    "                                                      means=statistics[\"mean\"],\n",
    "                                                      stds=statistics[\"std\"],\n",
    "                                                  return_only_image=True,\n",
    "                                                      )\n",
    "\n",
    "validation_dataset = DatasetGeneratorPreprocessedH5(path_to_data=path_to_data,\n",
    "                                                           set_indx=validation_indx,\n",
    "                                                           scaling_factor=scaling_factor,\n",
    "                                                           reshape_size=reshape_size,\n",
    "                                                           transform=test_transform,\n",
    "                                                           data_map=data_map,\n",
    "                                                           only_channels=only_channels,\n",
    "                                                           num_channels=num_channels,\n",
    "                                                           means=statistics[\"mean\"],\n",
    "                                                           stds=statistics[\"std\"],\n",
    "                                                       return_only_image=True,\n",
    "                                                           )\n",
    "\n",
    "test_dataset = DatasetGeneratorPreprocessedH5(path_to_data=path_to_data,\n",
    "                                                     set_indx=test_indx,\n",
    "                                                     scaling_factor=scaling_factor,\n",
    "                                                     reshape_size=reshape_size,\n",
    "                                                     transform=test_transform,\n",
    "                                                     data_map=data_map,\n",
    "                                                     only_channels=only_channels,\n",
    "                                                     num_channels=num_channels,\n",
    "                                                     means=statistics[\"mean\"],\n",
    "                                                     stds=statistics[\"std\"],\n",
    "                                                 return_only_image=True,\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrscheduler = LRScheduler(\n",
    "    policy='StepLR', step_size=7, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Checkpoint(\n",
    "    f_params='wbc_net_all_.pth', monitor='valid_loss_best', dirname='models')\n",
    "train_end_cp = TrainEndCheckpoint(f_params='final_wbc_net_all_.pth', dirname='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.FloatTensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialite and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    PretrainedModel, \n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    #criterion__weight=class_weights,\n",
    "    lr=0.001,\n",
    "    batch_size=64,\n",
    "    max_epochs=10,\n",
    "    module__output_features=num_classes,\n",
    "    module__num_classes=num_classes,\n",
    "    module__num_channels=num_channels, \n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__momentum=0.9,\n",
    "    iterator_train__shuffle=False,\n",
    "    iterator_train__num_workers=2,\n",
    "    iterator_valid__shuffle=False,\n",
    "    iterator_valid__num_workers=2,\n",
    "    callbacks=[lrscheduler, checkpoint, train_end_cp],\n",
    "    train_split=predefined_split(validation_dataset),\n",
    "    #device='cuda' # comment to train on cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.save_params(f_params='final_wbc_net_all_.pth')\n",
    "model = PretrainedModel(num_classes, num_channels)\n",
    "checkpoint = torch.load('models/wbc_net_all_.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0.\n",
    "total = 0.\n",
    "y_true = list()\n",
    "y_pred = list()\n",
    "y_true_proba = list()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device).float(), data[1].to(device).long()\n",
    "        #Tracer()()\n",
    "        outputs = model(inputs)\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        true_proba = np.array([j[i] for (i,j) in zip(pred, outputs)])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (labels.reshape(-1) == predicted).sum().item()\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "            y_true_proba.append(true_proba[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_targets = [c.decode(\"utf-8\") for c in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=class_names_targets, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save results\n",
    "model_name = \"wbc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-Permutation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time()\n",
    "f1_score_original = f1_score(y_true, y_pred, average=None, labels=np.arange(num_classes))\n",
    "min_mean_dif = 1.0\n",
    "candidate = 0\n",
    "shuffle_times = 5\n",
    "df_all = pd.DataFrame([], columns=class_names_targets)\n",
    "for c in range(num_channels):\n",
    "    f1_score_diff_from_original_per_channel_per_shuffle = []\n",
    "    for s in range(shuffle_times):\n",
    "        dataset = DatasetGeneratorPreprocessedH5(path_to_data=path_to_data,\n",
    "                                                      set_indx=test_indx,\n",
    "                                                      scaling_factor=scaling_factor,\n",
    "                                                     reshape_size=reshape_size,\n",
    "                                                      transform=test_transform,\n",
    "                                                      data_map=data_map,\n",
    "                                                      only_channels=only_channels,\n",
    "                                                      num_channels=num_channels,\n",
    "                                                      means=statistics[\"mean\"],\n",
    "                                                      stds=statistics[\"std\"],\n",
    "                                                    channels_to_shuffle=[c]\n",
    "                                                      )\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "        y_true = list()\n",
    "        y_pred = list()\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader:\n",
    "                inputs, labels = data[\"image\"].to(device).float(), data[\"label\"].to(device).reshape(-1).long()\n",
    "                outputs = model(inputs)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                for i in range(len(pred)):\n",
    "                    y_true.append(labels[i].item())\n",
    "                    y_pred.append(pred[i].item())\n",
    "            f1_score_per_channel = f1_score(y_true, y_pred, average=None, labels=np.arange(num_classes))\n",
    "            f1_score_diff_from_original_per_channel_per_shuffle.append(f1_score_original - f1_score_per_channel)\n",
    "    mean_along_columns = np.mean(f1_score_diff_from_original_per_channel_per_shuffle, axis=0)\n",
    "    mean_dif = np.mean(mean_along_columns)\n",
    "    if mean_dif < min_mean_dif and mean_dif > 0 and not only_channels[c]:\n",
    "        min_mean_dif = mean_dif\n",
    "        candidate = only_channels[c]\n",
    "    df_diff = pd.DataFrame(np.atleast_2d(f1_score_diff_from_original_per_channel_per_shuffle), columns=CLASS_NAMES)\n",
    "    df_mean_diff = pd.DataFrame(np.atleast_2d(mean_along_columns), columns=CLASS_NAMES)\n",
    "    df_all = pd.concat([df_all, df_mean_diff], ignore_index=True, sort=False)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax = df_diff.boxplot()\n",
    "    ax.set_xticklabels(class_names_targets, rotation=45)\n",
    "    fig.savefig(os.path.join(\"results\",model_name, \"{}-shuffle_method-model-{}-channel-{}.png\".format(dataset_name, str(model_name), str(only_channels[c]))))\n",
    "print(\"Candidate channel is {}\".format(candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(channels[np.asarray(only_channels)], df_all.T.mean(), color='Grey')\n",
    "plt.savefig(os.path.join(\"results\",model_name, \"{}-pixel-permutation-method-model-all-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_pixel_permutation = pd.DataFrame(data={'channels': channels[np.asarray(only_channels)], 'importance': df_all.T.mean().to_numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_pixel_permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the method with AOPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aopc(channel_ranking, method='', ascending=True, perturb=False, plot=True):\n",
    "    #channel_ranking = pd.DataFrame(data={\"channels\":channels_ranking, \"importance\": importance})\n",
    "    sorted_channels = channel_ranking.sort_values(by=\"importance\", ascending=ascending)\n",
    "    channels_to_permute=[]\n",
    "    differences = []\n",
    "    # calculate (f^0 - f^k)\n",
    "    for i in range(len(sorted_channels)):\n",
    "        channels_to_permute.append(np.where(channels==sorted_channels.iloc[i][\"channels\"])[0][0])\n",
    "        dataset_ = DatasetGeneratorPreprocessedH5(path_to_data=path_to_data,\n",
    "                                                          set_indx=test_indx,\n",
    "                                                          scaling_factor=scaling_factor,\n",
    "                                                         reshape_size=reshape_size,\n",
    "                                                          transform=test_transform,\n",
    "                                                          data_map=data_map,\n",
    "                                                          only_channels=only_channels,\n",
    "                                                          num_channels=num_channels,\n",
    "                                                          means=statistics[\"mean\"],\n",
    "                                                          stds=statistics[\"std\"],\n",
    "                                                        channels_to_shuffle=channels_to_permute,\n",
    "                                                        return_only_image=True,\n",
    "                                                    perturb=perturb)\n",
    "        dataloader_ = DataLoader(dataset_,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "        \n",
    "        y_true_permut_proba = list()\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader_:\n",
    "                inputs, labels = data[0].to(device).float(), data[1].to(device).long()\n",
    "                outputs = model(inputs)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                permut_proba = np.array([j[i] for (i,j) in zip(pred, outputs)])\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                for i in range(len(pred)):\n",
    "                    y_true_permut_proba.append(permut_proba[i].item())\n",
    "        differences.append(y_true_proba-np.array(y_true_permut_proba))\n",
    "    stacked_diff = np.stack(differences)\n",
    "    # calculate summ(f^0-f^k)\n",
    "    diff_accumulated = []\n",
    "    for idx, diff in enumerate(stacked_diff):\n",
    "        if idx==0:\n",
    "            diff_accumulated.append(stacked_diff[idx])\n",
    "        else:\n",
    "            diff_accumulated.append(diff_accumulated[idx-1] + stacked_diff[idx])\n",
    "    diff_accumulated = np.stack(diff_accumulated)\n",
    "    # mean over the test set\n",
    "    diff_accumulated_mean = np.mean(diff_accumulated, axis=-1)\n",
    "    # divide by 1/L+1\n",
    "    diff_accumulated_mean_norm = np.array([])\n",
    "    for ix, d in enumerate(diff_accumulated_mean):\n",
    "        diff_accumulated_mean_norm = np.append(diff_accumulated_mean_norm, d/(ix+1))\n",
    "    # insert (0,0)\n",
    "    diff_accumulated_mean_norm_started_from_0 = np.insert(diff_accumulated_mean_norm,0,0.0)\n",
    "    # plot line\n",
    "    if plot:\n",
    "        x = np.arange(len(diff_accumulated_mean_norm_started_from_0))\n",
    "        plt.xlabel(\"permutation steps\")\n",
    "        plt.ylabel(\"AOPC\")\n",
    "        plt.plot(x, diff_accumulated_mean_norm_started_from_0, color =\"red\")\n",
    "        #plt.show()\n",
    "        plt.savefig(os.path.join(\"results\",model_name, \"{}-aopc-{}-{}.svg\".format(dataset_name, method, str(\"resnet_all\"))))\n",
    "    return diff_accumulated_mean_norm_started_from_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pixel_permutated_perturb_reverse = calculate_aopc(channel_ranking_pixel_permutation, method='pixel-permutation-perturb', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation by methods from captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Occlusion, DeepLift, IntegratedGradients, LRP\n",
    "from time import process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time()\n",
    "ablator = Occlusion(model)\n",
    "dataset = DatasetGeneratorPreprocessedH5(path_to_data=path_to_data,\n",
    "                                                      set_indx=test_indx,\n",
    "                                                      scaling_factor=scaling_factor,\n",
    "                                                     reshape_size=reshape_size,\n",
    "                                                      transform=test_transform,\n",
    "                                                      data_map=data_map,\n",
    "                                                      only_channels=only_channels,\n",
    "                                                      num_channels=num_channels,\n",
    "                                                      means=statistics[\"mean\"],\n",
    "                                                      stds=statistics[\"std\"]\n",
    "                                                      )\n",
    "dataloader = DataLoader(dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "\n",
    "heatmaps = torch.empty(0, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data[\"image\"].to(device).float(), data[\"label\"].to(device).reshape(-1).long()\n",
    "        attr = ablator.attribute(inputs, target=labels, sliding_window_shapes=(1,3,3))\n",
    "        heatmaps = torch.cat((heatmaps, torch.from_numpy(np.percentile(torch.flatten(attr, start_dim=-2).cpu().numpy(), q=98, axis=-1)).to(dev)))\n",
    "heatmaps_mean = torch.mean(heatmaps, dim=0)\n",
    "plt.bar(channels[np.asarray(only_channels)], heatmaps_mean.cpu(), color='grey')\n",
    "plt.savefig(os.path.join(\"results\",model_name, \"{}-occl_method-model-98-percentile-{}.png\".format(dataset_name, str(\"resnet_all\"))))\n",
    "\n",
    "t1_stop = process_time()\n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "   \n",
    "print(\"Elapsed time during the whole program in seconds:\",\n",
    "                                         t1_stop-t1_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_occlusion = pd.DataFrame(data={'channels': channels[np.asarray(only_channels)], 'importance': heatmaps_mean.cpu().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pixel_ocll_perturb_reverse = calculate_aopc(channel_ranking_occlusion, method='pixel-occlusion-perturb-reverse', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time()\n",
    "ablator = DeepLift(model)\n",
    "\n",
    "test_dataset = DatasetGeneratorPreprocessedH5(path_to_data=path_to_data,\n",
    "                                                     set_indx=test_indx,\n",
    "                                                     scaling_factor=scaling_factor,\n",
    "                                                     reshape_size=reshape_size,\n",
    "                                                     transform=test_transform,\n",
    "                                                     data_map=data_map,\n",
    "                                                     only_channels=only_channels,\n",
    "                                                     num_channels=num_channels,\n",
    "                                                     means=statistics[\"mean\"],\n",
    "                                                     stds=statistics[\"std\"]\n",
    "                                                     )\n",
    "testloader = DataLoader(test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "\n",
    "heatmaps_deeplift = torch.empty(0, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[\"image\"].to(device).float(), data[\"label\"].to(device).reshape(-1).long()\n",
    "        # baselines=torch.zeros(inputs.shape).to(dev)\n",
    "        attr = ablator.attribute(inputs, target=labels)\n",
    "        heatmaps_deeplift = torch.cat((heatmaps_deeplift,  torch.from_numpy(np.percentile(torch.flatten(attr, start_dim=-2).cpu().numpy(), q=98, axis=-1)).to(device)))\n",
    "heatmaps_deeplift_mean = torch.mean(heatmaps_deeplift, dim=0)\n",
    "plt.bar(channels, heatmaps_deeplift_mean.cpu(), color='grey')\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-deeplift_method-model-98-percentile-{}.png\".format(dataset_name, str(\"resnet_all\"))))\n",
    "\n",
    "t1_stop = process_time()\n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "   \n",
    "print(\"Elapsed time during the whole program in seconds:\",\n",
    "                                         t1_stop-t1_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_deep_lift = pd.DataFrame(data={'channels': channels[np.asarray(only_channels)], 'importance': heatmaps_deeplift_mean.cpu().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_deep_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_deep_lift_perturb_reverse = calculate_aopc(channel_ranking_deep_lift, method='deep-lift-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_random = pd.DataFrame(data={'channels': channels[np.asarray(only_channels)], 'importance': np.random.randint(12, size=12)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_random_perturb_reverse = calculate_aopc(channel_ranking_random, method='random-perturb-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(res_random_perturb_reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 13})\n",
    "got_label=False\n",
    "plt.plot(x, res_deep_lift_perturb_reverse, label  = \"Channel-wise DeepLift\", color=\"orange\")\n",
    "plt.plot(x, res_pixel_ocll_perturb_reverse, label  = \"Channel-wise Occlusion\", color=\"green\")\n",
    "plt.plot(x, res_pixel_permutated_perturb_reverse, label  = \"Pixel-Permutation\", color=\"red\")\n",
    "plt.plot(x, res_random_perturb_reverse, label  = \"Random Baseline\", color=\"blue\")\n",
    "plt.xlabel('Perturbation steps')\n",
    "plt.ylabel('AOPC')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-{}.svg\".format(dataset_name, str(\"resnet_all\"))))\n",
    "#plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the random channel ranking 100 times to estimate the lower und upper bound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=2.576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_border = mean + (z * (std / np.sqrt(len(random_rankings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_border = mean - (z * (std / np.sqrt(len(random_rankings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    channel_ranking_random = pd.DataFrame(data={'channels': channels[np.asarray(only_channels)], 'importance': np.random.randint(12, size=12)})\n",
    "    random_rankings.append(calculate_aopc(channel_ranking_random, method='random-perturb-reverse', ascending=False, perturb=True, plot=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 13})\n",
    "got_label=False\n",
    "for ranking in random_rankings:\n",
    "    if not got_label:\n",
    "        plt.plot(x, ranking, label  = \"Random Baseline\", color=\"grey\", linewidth=0.5, alpha=0.1)\n",
    "        got_label=True\n",
    "    else:\n",
    "        plt.plot(x, ranking, color=\"grey\", linewidth=0.5, alpha=0.1)\n",
    "plt.plot(x, res_deep_lift_perturb_reverse, label  = \"Channel-wise DeepLift\", color=\"orange\")\n",
    "plt.plot(x, res_pixel_ocll_perturb_reverse, label  = \"Channel-wise Occlusion\", color=\"green\")\n",
    "plt.plot(x, res_pixel_permutated_perturb_reverse, label  = \"Pixel-Permutation\", color=\"red\")\n",
    "#plt.plot(x, res_random_perturb_reverse, label  = \"Random Baseline\", color=\"blue\")\n",
    "plt.xlabel('Perturbation steps')\n",
    "plt.ylabel('AOPC')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-1010-{}.svg\".format(dataset_name, str(\"resnet_all\"))))\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-1010-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newiflai",
   "language": "python",
   "name": "newiflai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}