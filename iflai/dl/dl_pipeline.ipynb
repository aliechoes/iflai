{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from imageio import imread\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from util import calculate_weights, train_validation_test_split, get_statistics\n",
    "from dataset import DatasetGenerator\n",
    "from custom_transforms import ShuffleChannel\n",
    "from IPython.core.debugger import Tracer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# Compare Algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from models import PretrainedModel, resnet18\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "import torch.optim as optim\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iflai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all random seeds to the specific value, so the results are more reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"wbc\"\n",
    "selected_channels = np.arange(12)\n",
    "path_to_data =\"..\\..\\data/WBC\"\n",
    "model_dir = \"models_remote\"\n",
    "scaling_factor = 4095.\n",
    "reshape_size = 64\n",
    "num_channels = len(selected_channels)\n",
    "train_transform = [\n",
    "         transforms.RandomVerticalFlip(),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(45)\n",
    "        ]\n",
    "test_transform = [ ]\n",
    "channels =np.asarray([ \"Ch\" + str(i) for i in selected_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 2\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "metadata = iflai.metadata_generator(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = metadata[\"label\"] != \"unknown\"\n",
    "metadata = metadata.loc[indx,:]\n",
    "metadata = metadata.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, validation_index, test_index = train_validation_test_split(metadata.index, metadata[\"label\"], random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = dict(zip(sorted(set(metadata.loc[train_index, \"label\"])), np.arange(len(set(metadata.loc[train_index, \"label\"])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversamle and use class weights for imbalance data / Skip if not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [label_map.get(metadata.loc[i, \"label\"]) for i in train_index]\n",
    "weights = calculate_weights(y_train)\n",
    "oversample = RandomOverSampler(random_state=seed_value, sampling_strategy='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, y_train = oversample.fit_resample(np.asarray(train_index).reshape(-1, 1), y_train)\n",
    "train_index = train_index.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate statistics of train set and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetGenerator(metadata=metadata.loc[train_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor,\n",
    "                                 reshape_size=reshape_size,\n",
    "                                 transform=transforms.Compose(train_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = get_statistics(train_loader, selected_channels=selected_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform.append(transforms.Normalize(mean=statistics[\"mean\"],\n",
    "                         std=statistics[\"std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform.append(transforms.Normalize(mean=statistics[\"mean\"],\n",
    "                         std=statistics[\"std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetGenerator(metadata=metadata.loc[train_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor, \n",
    "                                 reshape_size=reshape_size,\n",
    "                                 transform= transforms.Compose(train_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = DatasetGenerator(metadata=metadata.loc[validation_index,:],\n",
    "                                      label_map=label_map,\n",
    "                                      selected_channels=selected_channels,\n",
    "                                      scaling_factor=scaling_factor,\n",
    "                                      reshape_size=reshape_size,\n",
    "                                      transform=transforms.Compose(test_transform))\n",
    "test_dataset = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                                label_map=label_map,\n",
    "                                selected_channels=selected_channels,\n",
    "                                scaling_factor=scaling_factor,\n",
    "                                reshape_size=reshape_size,\n",
    "                                transform=\n",
    "                                transforms.Compose(test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrscheduler = LRScheduler(\n",
    "    policy='StepLR', step_size=7, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Checkpoint(\n",
    "    f_params='wbc_net_all_.pth', monitor='valid_loss_best', dirname='models')\n",
    "train_end_cp = TrainEndCheckpoint(f_params='final_wbc_net_all_.pth', dirname='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.FloatTensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialite and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    PretrainedModel, \n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    #criterion__weight=class_weights,\n",
    "    lr=0.001,\n",
    "    batch_size=64,\n",
    "    max_epochs=10,\n",
    "    module__output_features=num_classes,\n",
    "    module__num_classes=num_classes,\n",
    "    module__num_channels=num_channels, \n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__momentum=0.9,\n",
    "    iterator_train__shuffle=False,\n",
    "    iterator_train__num_workers=2,\n",
    "    iterator_valid__shuffle=False,\n",
    "    iterator_valid__num_workers=2,\n",
    "    callbacks=[lrscheduler, checkpoint, train_end_cp],\n",
    "    train_split=predefined_split(validation_dataset),\n",
    "    #device='cuda' # comment to train on cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.save_params(f_params='final_wbc_net_all_.pth')\n",
    "model = PretrainedModel(num_classes, num_channels)\n",
    "checkpoint = torch.load('models/wbc_net_all_.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0.\n",
    "total = 0.\n",
    "y_true = list()\n",
    "y_pred = list()\n",
    "y_true_proba = list()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data[0].to(device).float(), data[1].to(device).long()\n",
    "        #Tracer()()\n",
    "        outputs = model(inputs)\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        true_proba = np.array([j[i] for (i,j) in zip(pred, outputs)])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (labels.reshape(-1) == predicted).sum().item()\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "            y_true_proba.append(true_proba[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_targets = [c.decode(\"utf-8\") for c in label_map.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=class_names_targets, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save results\n",
    "model_name = \"wbc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-Permutation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time()\n",
    "f1_score_original = f1_score(y_true, y_pred, average=None, labels=np.arange(num_classes))\n",
    "min_mean_dif = 1.0\n",
    "candidate = 0\n",
    "shuffle_times = 5\n",
    "df_all = pd.DataFrame([], columns=class_names_targets)\n",
    "for c in range(num_channels):\n",
    "    f1_score_diff_from_original_per_channel_per_shuffle = []\n",
    "    transform = test_transform.copy()\n",
    "    transform.append(ShuffleChannel(channels_to_shuffle=[c]))\n",
    "    for s in range(shuffle_times):\n",
    "        dataset = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor,\n",
    "                                 reshape_size=reshape_size,\n",
    "                                 transform=transforms.Compose(transform))\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "        y_true = list()\n",
    "        y_pred = list()\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader:\n",
    "                inputs, labels = data[0].to(device).float(), data[1].to(device).reshape(-1).long()\n",
    "                outputs = model(inputs)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                for i in range(len(pred)):\n",
    "                    y_true.append(labels[i].item())\n",
    "                    y_pred.append(pred[i].item())\n",
    "            f1_score_per_channel = f1_score(y_true, y_pred, average=None, labels=np.arange(num_classes))\n",
    "            f1_score_diff_from_original_per_channel_per_shuffle.append(f1_score_original - f1_score_per_channel)\n",
    "    mean_along_columns = np.mean(f1_score_diff_from_original_per_channel_per_shuffle, axis=0)\n",
    "    mean_dif = np.mean(mean_along_columns)\n",
    "    if mean_dif < min_mean_dif and mean_dif > 0 and not selected_channels[c]:\n",
    "        min_mean_dif = mean_dif\n",
    "        candidate = selected_channels[c]\n",
    "    df_diff = pd.DataFrame(np.atleast_2d(f1_score_diff_from_original_per_channel_per_shuffle), columns=class_names_targets)\n",
    "    df_mean_diff = pd.DataFrame(np.atleast_2d(mean_along_columns), columns=class_names_targets)\n",
    "    df_all = pd.concat([df_all, df_mean_diff], ignore_index=True, sort=False)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax = df_diff.boxplot()\n",
    "    ax.set_xticklabels(class_names_targets, rotation=45)\n",
    "    fig.savefig(os.path.join(\"results\",model_name, \"{}-shuffle_method-model-{}-channel-{}.png\".format(dataset_name, str(model_name), str(selected_channels[c]))))\n",
    "print(\"Candidate channel is {}\".format(candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(channels[selected_channels], df_all.T.mean(), color='Grey')\n",
    "plt.savefig(os.path.join(\"results\",model_name, \"{}-pixel-permutation-method-model-all-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_pixel_permutation = pd.DataFrame(data={'channels': channels[np.asarray(selected_channels)], 'importance': df_all.T.mean().to_numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_pixel_permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the method with AOPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aopc(channel_ranking, method='', ascending=False, plot=True, perturb=False):\n",
    "    #channel_ranking = pd.DataFrame(data={\"channels\":channels_ranking, \"importance\": importance})\n",
    "    sorted_channels = channel_ranking.sort_values(by=\"importance\", ascending=ascending)\n",
    "    channels_to_permute=[]\n",
    "    differences = []\n",
    "    # calculate (f^0 - f^k)\n",
    "    for i in range(len(sorted_channels)):\n",
    "        channels_to_permute.append(np.where(channels==sorted_channels.iloc[i][\"channels\"])[0][0])\n",
    "        transform = test_transform.copy()\n",
    "        transform.append(ShuffleChannel(channels_to_shuffle=channels_to_permute, perturb=perturb))\n",
    "        dataset_ = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor,\n",
    "                                 reshape_size=reshape_size,\n",
    "                                 transform=transforms.Compose(transform))\n",
    "        dataloader_ = DataLoader(dataset_,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "        \n",
    "        y_true_permut_proba = list()\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader_:\n",
    "                inputs, labels = data[0].to(device).float(), data[1].to(device).long()\n",
    "                outputs = model(inputs)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                permut_proba = np.array([j[i] for (i,j) in zip(pred, outputs)])\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                for i in range(len(pred)):\n",
    "                    y_true_permut_proba.append(permut_proba[i].item())\n",
    "        differences.append(y_true_proba-np.array(y_true_permut_proba))\n",
    "    stacked_diff = np.stack(differences)\n",
    "    # calculate summ(f^0-f^k)\n",
    "    diff_accumulated = []\n",
    "    for idx, diff in enumerate(stacked_diff):\n",
    "        if idx==0:\n",
    "            diff_accumulated.append(stacked_diff[idx])\n",
    "        else:\n",
    "            diff_accumulated.append(diff_accumulated[idx-1] + stacked_diff[idx])\n",
    "    diff_accumulated = np.stack(diff_accumulated)\n",
    "    # mean over the test set\n",
    "    diff_accumulated_mean = np.mean(diff_accumulated, axis=-1)\n",
    "    # divide by 1/L+1\n",
    "    diff_accumulated_mean_norm = np.array([])\n",
    "    for ix, d in enumerate(diff_accumulated_mean):\n",
    "        diff_accumulated_mean_norm = np.append(diff_accumulated_mean_norm, d/(ix+1))\n",
    "    # insert (0,0)\n",
    "    diff_accumulated_mean_norm_started_from_0 = np.insert(diff_accumulated_mean_norm,0,0.0)\n",
    "    # plot line\n",
    "    if plot:\n",
    "        x = np.arange(len(diff_accumulated_mean_norm_started_from_0))\n",
    "        plt.xlabel(\"permutation steps\")\n",
    "        plt.ylabel(\"AOPC\")\n",
    "        plt.plot(x, diff_accumulated_mean_norm_started_from_0, color =\"red\")\n",
    "        #plt.show()\n",
    "        plt.savefig(os.path.join(\"results\",model_name, \"{}-aopc-{}-{}.svg\".format(dataset_name, method, str(\"resnet_all\"))))\n",
    "    return diff_accumulated_mean_norm_started_from_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pixel_permutated_perturb_reverse = calculate_aopc(channel_ranking_pixel_permutation, method='pixel-permutation-perturb', perturb=True, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation by methods from captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import (\n",
    "    GuidedGradCam,\n",
    "    DeepLift,\n",
    "    Saliency,\n",
    "    DeepLiftShap,\n",
    "    GradientShap,\n",
    "    InputXGradient,\n",
    "    IntegratedGradients,\n",
    "    GuidedBackprop,\n",
    "    Deconvolution,\n",
    "    Occlusion,\n",
    "    FeaturePermutation,\n",
    "    ShapleyValueSampling,\n",
    "    Lime,\n",
    "    KernelShap,\n",
    "    LRP\n",
    ")\n",
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interpretation_method(ablator, method_name, require_baseline=False, require_sliding_window=False):\n",
    "    t1_start = process_time()\n",
    "    dataset = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                                     label_map=label_map,\n",
    "                                     selected_channels=selected_channels,\n",
    "                                     scaling_factor=scaling_factor,\n",
    "                                     reshape_size=reshape_size,\n",
    "                                     transform=transforms.Compose(test_transform))\n",
    "    testloader = DataLoader(test_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=num_workers)\n",
    "\n",
    "    heatmaps = torch.empty(0, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data[0].to(device).float(), data[1].to(device).reshape(-1).long()\n",
    "            if require_baseline:\n",
    "                baselines=torch.zeros(inputs.shape).to(device)\n",
    "                attr = ablator.attribute(inputs, target=labels, baselines=baselines)\n",
    "            elif require_sliding_window:\n",
    "                attr = ablator.attribute(inputs, target=labels, sliding_window_shapes=(1,3,3))\n",
    "            else:\n",
    "                attr = ablator.attribute(inputs, target=labels)\n",
    "            heatmaps = torch.cat((heatmaps,  torch.from_numpy(np.percentile(torch.flatten(attr, start_dim=-2).cpu().numpy(), q=50, axis=-1)).to(device)))\n",
    "    heatmaps_mean = torch.mean(heatmaps, dim=0)\n",
    "    plt.bar(channels, heatmaps_mean.cpu(), color='grey')\n",
    "    plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-{}_method-model-50-percentile-{}.png\".format(dataset_name, method_name, str(\"resnet_all\"))))\n",
    "\n",
    "    t1_stop = process_time()\n",
    "    print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "\n",
    "    print(\"Elapsed time during the whole program in seconds:\",\n",
    "                                             t1_stop-t1_start)\n",
    "    return heatmaps_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occl = Occlusion(model)\n",
    "heatmaps_occl_mean = run_interpretation_method(occl, 'Occlusion', require_sliding_window=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_occlusion = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_occl_mean.cpu().numpy()})\n",
    "res_pixel_occl_perturb_reverse = calculate_aopc(channel_ranking_occlusion, method='occlusion-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DeepLift(model)\n",
    "heatmaps_deeplift_mean = run_interpretation_method(dl, 'dl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_deep_lift = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_deeplift_mean.cpu().numpy()})\n",
    "res_deep_lift_perturb_reverse = calculate_aopc(channel_ranking_deep_lift, method='deep-lift-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency = Saliency(model)\n",
    "heatmaps_saliency_mean = run_interpretation_method(saliency, 'saliency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_saliency = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_saliency_mean.cpu().numpy()})\n",
    "res_saliency_perturb_reverse = calculate_aopc(channel_ranking_saliency, method='saliency-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "heatmaps_integrated_gradient_mean = run_interpretation_method(ig, 'IntegratedGradients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_integrated_gradient = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_integrated_gradient_mean.cpu().numpy()})\n",
    "res_integrated_gradient_perturb_reverse = calculate_aopc(channel_ranking_integrated_gradient, method='integrated_gradient-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime = Lime(model)\n",
    "heatmaps_lime_mean = run_interpretation_method(lime, 'Lime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_lime = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_lime_mean.cpu().numpy()})\n",
    "res_lime_mean_perturb_reverse = calculate_aopc(channel_ranking_lime, method='lime-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv = Deconvolution(model)\n",
    "heatmaps_deconv_mean = run_interpretation_method(deconv, 'Deconvolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_deconv = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_deconv_mean.cpu().numpy()})\n",
    "res_deconv_mean_perturb_reverse = calculate_aopc(channel_ranking_deconv, method='deconv-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp = LRP(model)\n",
    "heatmaps_lrp_mean = run_interpretation_method(lrp, 'LRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_lrp = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_lrp_mean.cpu().numpy()})\n",
    "res_lrp_mean_perturb_reverse = calculate_aopc(channel_ranking_lrp, method='lrp-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guided GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_cram = GuidedGradCam(model, model.model.layer1)\n",
    "heatmaps_gradcam_mean = run_interpretation_method(guided_cram, 'GuidedGradCam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_gradcam = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_gradcam_mean.cpu().numpy()})\n",
    "res_gradcam_mean_perturb_reverse = calculate_aopc(channel_ranking_gradcam, method='gradcam-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guided Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_backprop = GuidedBackprop(model)\n",
    "heatmaps_backprop_mean = run_interpretation_method(guided_backprop, 'GuidedBackprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_backprop = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_backprop_mean.cpu().numpy()})\n",
    "res_backprop_mean_perturb_reverse = calculate_aopc(channel_ranking_backprop, method='guided_backprop-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepLiftShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlshap = DeepLiftShap(model)\n",
    "heatmaps_dlshap_mean = run_interpretation_method(dlshap, 'DeepLiftShap', require_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_dlshap = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_dlshap_mean.cpu().numpy()})\n",
    "res_dlshap_mean_perturb_reverse = calculate_aopc(channel_ranking_dlshap, method='dlshap-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapley Value Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley = ShapleyValueSampling(net)\n",
    "heatmaps_shapley_mean = run_interpretation_method(shapley, 'Shapley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_shapley = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_shapley_mean.cpu().numpy()})\n",
    "res_shapley_mean_perturb_reverse = calculate_aopc(channel_ranking_shapley, method='shapley-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_perm = FeaturePermutation(net)\n",
    "heatmaps_feat_perm_mean = run_interpretation_method(feat_perm, 'FeaturePermutation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_feat_perm = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_feat_perm_mean.cpu().numpy()})\n",
    "res_feat_perm_mean_perturb_reverse = calculate_aopc(channel_ranking_feat_perm, method='feat_permut-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_abl = FeatureAblation(net)\n",
    "heatmaps_feat_abl_mean = run_interpretation_method(feat_abl, 'FeatureAblation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_feat_abl = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_feat_abl_mean.cpu().numpy()})\n",
    "res_feat_abl_mean_perturb_reverse = calculate_aopc(channel_ranking_feat_abl, method='feat_abl-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradshap = GradientShap(model)\n",
    "heatmaps_gradshap_mean = run_interpretation_method(gradshap, 'GradientShap', require_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_gradshap = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_gradshap_mean.cpu().numpy()})\n",
    "res_gradshap_mean_perturb_reverse = calculate_aopc(channel_ranking_gradshap, method='gradshap-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_random = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': np.random.randint(len(selected_channels), size=len(selected_channels))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_random_perturb_reverse = calculate_aopc(channel_ranking_random, method='random-perturb-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(res_random_perturb_reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('hsv', 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 13})\n",
    "got_label=False\n",
    "plt.plot(x, res_deep_lift_perturb_reverse, label  = \"Channel-wise DeepLift\", c=cmap(0))\n",
    "plt.plot(x, res_pixel_ocll_perturb_reverse, label  = \"Channel-wise Occlusion\", c=cmap(1))\n",
    "plt.plot(x, res_pixel_permutated_perturb_reverse, label  = \"Pixel-Permutation\", c=cmap(2))\n",
    "#plt.plot(x, res_random_perturb_reverse, label  = \"Random Baseline\", color=\"blue\")\n",
    "plt.plot(x, res_saliency_perturb_reverse, label  = \"Channel-wise Saliency\", c=cmap(3))\n",
    "plt.plot(x, res_integrated_gradient_perturb_reverse, label  = \"Channel-wise Integrated Gradient\", c=cmap(4))\n",
    "plt.plot(x, res_lime_mean_perturb_reverse, label  = \"Channel-wise LIME\", c=cmap(5))\n",
    "plt.plot(x, res_deconv_mean_perturb_reverse, label  = \"Channel-wise Deconvolution\", c=cmap(6))\n",
    "plt.plot(x, res_lrp_mean_perturb_reverse, label  = \"Channel-wise LRP\", c=cmap(7))\n",
    "plt.plot(x, res_gradshap_mean_perturb_reverse, label=\"Channel-wise GradientShap\", c=cmap(8))\n",
    "plt.plot(x, res_feat_abl_mean_perturb_reverse, label=\"Channel-wise Feature Ablation\", c=cmap(9))\n",
    "plt.plot(x, res_feat_perm_mean_perturb_reverse, label=\"Channel-wise Feature Permutation\", c=cmap(10))\n",
    "plt.plot(x, res_shapley_mean_perturb_reverse, label=\"Channel-wise Shapley\", c=cmap(11))\n",
    "plt.plot(x, res_dlshap_mean_perturb_reverse, label=\"Channel-wise DeepLiftShap\", c=cmap(12))\n",
    "plt.plot(x, res_backprop_mean_perturb_reverse, label=\"Channel-wise Guided Backprop\", c=cmap(13))\n",
    "plt.plot(x, res_gradcam_mean_perturb_reverse, label=\"Channel-wise Guided GradCam\", c=cmap(14))\n",
    "plt.xlabel('Perturbation steps')\n",
    "plt.ylabel('AOPC')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-{}.svg\".format(dataset_name, str(\"resnet_all\"))))\n",
    "#plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the random channel ranking 100 times to estimate the lower und upper bound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=2.576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_border = mean + (z * (std / np.sqrt(len(random_rankings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_border = mean - (z * (std / np.sqrt(len(random_rankings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    channel_ranking_random = pd.DataFrame(data={'channels': channels[np.asarray(only_channels)], 'importance': np.random.randint(len(selected_channels), size=len(selected_channels))})\n",
    "    random_rankings.append(calculate_aopc(channel_ranking_random, method='random-perturb-reverse', ascending=False, perturb=True, plot=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 13})\n",
    "got_label=False\n",
    "for ranking in random_rankings:\n",
    "    if not got_label:\n",
    "        plt.plot(x, ranking, label  = \"Random Baseline\", color=\"grey\", linewidth=0.5, alpha=0.1)\n",
    "        got_label=True\n",
    "    else:\n",
    "        plt.plot(x, ranking, color=\"grey\", linewidth=0.5, alpha=0.1)\n",
    "plt.plot(x, res_deep_lift_perturb_reverse, label  = \"Channel-wise DeepLift\", color=\"orange\")\n",
    "plt.plot(x, res_pixel_ocll_perturb_reverse, label  = \"Channel-wise Occlusion\", color=\"green\")\n",
    "plt.plot(x, res_pixel_permutated_perturb_reverse, label  = \"Pixel-Permutation\", color=\"red\")\n",
    "#plt.plot(x, res_random_perturb_reverse, label  = \"Random Baseline\", color=\"blue\")\n",
    "plt.xlabel('Perturbation steps')\n",
    "plt.ylabel('AOPC')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-1010-{}.svg\".format(dataset_name, str(\"resnet_all\"))))\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-1010-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newiflai",
   "language": "python",
   "name": "newiflai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}