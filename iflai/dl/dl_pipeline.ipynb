{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from imageio import imread\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from util import calculate_weights, train_validation_test_split, get_statistics\n",
    "from dataset import DatasetGenerator\n",
    "from custom_transforms import ShuffleChannel\n",
    "from IPython.core.debugger import Tracer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# Compare Algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from models import PretrainedModel, resnet18\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "import torch.optim as optim\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iflai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all random seeds to the specific value, so the results are more reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"wbc\"\n",
    "selected_channels = np.arange(12)\n",
    "path_to_data =\"..\\..\\data/WBC\"\n",
    "model_dir = \"models_remote\"\n",
    "scaling_factor = 4095.\n",
    "reshape_size = 64\n",
    "num_channels = len(selected_channels)\n",
    "train_transform = [\n",
    "         transforms.RandomVerticalFlip(),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(45)\n",
    "        ]\n",
    "test_transform = [ ]\n",
    "channels =np.asarray([ \"Ch\" + str(i) for i in selected_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 2\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "metadata = iflai.metadata_generator(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = metadata[\"label\"] != \"unknown\"\n",
    "metadata = metadata.loc[indx,:]\n",
    "metadata = metadata.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, validation_index, test_index = train_validation_test_split(metadata.index, metadata[\"label\"], random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = dict(zip(sorted(set(metadata.loc[train_index, \"label\"])), np.arange(len(set(metadata.loc[train_index, \"label\"])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversamle and use class weights for imbalance data / Skip if not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [label_map.get(metadata.loc[i, \"label\"]) for i in train_index]\n",
    "weights = calculate_weights(y_train)\n",
    "oversample = RandomOverSampler(random_state=seed_value, sampling_strategy='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, y_train = oversample.fit_resample(np.asarray(train_index).reshape(-1, 1), y_train)\n",
    "train_index = train_index.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate statistics of train set and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetGenerator(metadata=metadata.loc[train_index,:], label_map=label_map, selected_channels=selected_channels, scaling_factor=scaling_factor, transform=transforms.Compose(train_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = get_statistics(train_loader, selected_channels=np.arange(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform.append(transforms.Normalize(mean=statistics[\"mean\"],\n",
    "                         std=statistics[\"std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform.append(transforms.Normalize(mean=statistics[\"mean\"],\n",
    "                         std=statistics[\"std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetGenerator(metadata=metadata.loc[train_index,:],\n",
    "                                 label_map=label_map,\n",
    "                                 selected_channels=selected_channels,\n",
    "                                 scaling_factor=scaling_factor, \n",
    "                                 transform= transforms.Compose(train_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = DatasetGenerator(metadata=metadata.loc[validation_index,:], label_map=label_map, selected_channels=np.arange(12), scaling_factor=scaling_factor, transform= transforms.Compose(test_transform))\n",
    "test_dataset = DatasetGenerator(metadata=metadata.loc[test_index,:], label_map=label_map, selected_channels=np.arange(12), scaling_factor=scaling_factor, transform= transforms.Compose(test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrscheduler = LRScheduler(\n",
    "    policy='StepLR', step_size=7, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Checkpoint(\n",
    "    f_params='wbc_net_all_.pth', monitor='valid_loss_best', dirname='models')\n",
    "train_end_cp = TrainEndCheckpoint(f_params='final_wbc_net_all_.pth', dirname='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.FloatTensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialite and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    PretrainedModel, \n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    #criterion__weight=class_weights,\n",
    "    lr=0.001,\n",
    "    batch_size=64,\n",
    "    max_epochs=10,\n",
    "    module__output_features=num_classes,\n",
    "    module__num_classes=num_classes,\n",
    "    module__num_channels=num_channels, \n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__momentum=0.9,\n",
    "    iterator_train__shuffle=False,\n",
    "    iterator_train__num_workers=2,\n",
    "    iterator_valid__shuffle=False,\n",
    "    iterator_valid__num_workers=2,\n",
    "    callbacks=[lrscheduler, checkpoint, train_end_cp],\n",
    "    train_split=predefined_split(validation_dataset),\n",
    "    #device='cuda' # comment to train on cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.save_params(f_params='final_wbc_net_all_.pth')\n",
    "model = PretrainedModel(num_classes, num_channels)\n",
    "checkpoint = torch.load('models/wbc_net_all_.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0.\n",
    "total = 0.\n",
    "y_true = list()\n",
    "y_pred = list()\n",
    "y_true_proba = list()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data[0].to(device).float(), data[1].to(device).long()\n",
    "        #Tracer()()\n",
    "        outputs = model(inputs)\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        true_proba = np.array([j[i] for (i,j) in zip(pred, outputs)])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (labels.reshape(-1) == predicted).sum().item()\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "            y_true_proba.append(true_proba[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_targets = [c.decode(\"utf-8\") for c in label_map.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=class_names_targets, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save results\n",
    "model_name = \"wbc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-Permutation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time()\n",
    "f1_score_original = f1_score(y_true, y_pred, average=None, labels=np.arange(num_classes))\n",
    "min_mean_dif = 1.0\n",
    "candidate = 0\n",
    "shuffle_times = 5\n",
    "df_all = pd.DataFrame([], columns=class_names_targets)\n",
    "for c in range(num_channels):\n",
    "    f1_score_diff_from_original_per_channel_per_shuffle = []\n",
    "    transform = test_transform.copy()\n",
    "    transform.append(ShuffleChannel(channels_to_shuffle=[c]))\n",
    "    for s in range(shuffle_times):\n",
    "        dataset = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                                   label_map=label_map,\n",
    "                                   selected_channels=np.arange(12),\n",
    "                                   scaling_factor=scaling_factor,\n",
    "                                   transform=transforms.Compose(transform))\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "        y_true = list()\n",
    "        y_pred = list()\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader:\n",
    "                inputs, labels = data[0].to(device).float(), data[1].to(device).reshape(-1).long()\n",
    "                outputs = model(inputs)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                for i in range(len(pred)):\n",
    "                    y_true.append(labels[i].item())\n",
    "                    y_pred.append(pred[i].item())\n",
    "            f1_score_per_channel = f1_score(y_true, y_pred, average=None, labels=np.arange(num_classes))\n",
    "            f1_score_diff_from_original_per_channel_per_shuffle.append(f1_score_original - f1_score_per_channel)\n",
    "    mean_along_columns = np.mean(f1_score_diff_from_original_per_channel_per_shuffle, axis=0)\n",
    "    mean_dif = np.mean(mean_along_columns)\n",
    "    if mean_dif < min_mean_dif and mean_dif > 0 and not selected_channels[c]:\n",
    "        min_mean_dif = mean_dif\n",
    "        candidate = selected_channels[c]\n",
    "    df_diff = pd.DataFrame(np.atleast_2d(f1_score_diff_from_original_per_channel_per_shuffle), columns=class_names_targets)\n",
    "    df_mean_diff = pd.DataFrame(np.atleast_2d(mean_along_columns), columns=class_names_targets)\n",
    "    df_all = pd.concat([df_all, df_mean_diff], ignore_index=True, sort=False)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax = df_diff.boxplot()\n",
    "    ax.set_xticklabels(class_names_targets, rotation=45)\n",
    "    fig.savefig(os.path.join(\"results\",model_name, \"{}-shuffle_method-model-{}-channel-{}.png\".format(dataset_name, str(model_name), str(selected_channels[c]))))\n",
    "print(\"Candidate channel is {}\".format(candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(channels[selected_channels], df_all.T.mean(), color='Grey')\n",
    "plt.savefig(os.path.join(\"results\",model_name, \"{}-pixel-permutation-method-model-all-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_pixel_permutation = pd.DataFrame(data={'channels': channels[np.asarray(selected_channels)], 'importance': df_all.T.mean().to_numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_pixel_permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the method with AOPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aopc(channel_ranking, method='', ascending=False, plot=True, perturb=False):\n",
    "    #channel_ranking = pd.DataFrame(data={\"channels\":channels_ranking, \"importance\": importance})\n",
    "    sorted_channels = channel_ranking.sort_values(by=\"importance\", ascending=ascending)\n",
    "    channels_to_permute=[]\n",
    "    differences = []\n",
    "    # calculate (f^0 - f^k)\n",
    "    for i in range(len(sorted_channels)):\n",
    "        channels_to_permute.append(np.where(channels==sorted_channels.iloc[i][\"channels\"])[0][0])\n",
    "        transform = test_transform.copy()\n",
    "        transform.append(ShuffleChannel(channels_to_shuffle=channels_to_permute, perturb=perturb))\n",
    "        dataset_ = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                                    label_map=label_map,\n",
    "                                    selected_channels=np.arange(12),\n",
    "                                    transform= transforms.Compose(transform),\n",
    "                                    scaling_factor=scaling_factor)\n",
    "        dataloader_ = DataLoader(dataset_,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "        \n",
    "        y_true_permut_proba = list()\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader_:\n",
    "                inputs, labels = data[0].to(device).float(), data[1].to(device).long()\n",
    "                outputs = model(inputs)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                permut_proba = np.array([j[i] for (i,j) in zip(pred, outputs)])\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                for i in range(len(pred)):\n",
    "                    y_true_permut_proba.append(permut_proba[i].item())\n",
    "        differences.append(y_true_proba-np.array(y_true_permut_proba))\n",
    "    stacked_diff = np.stack(differences)\n",
    "    # calculate summ(f^0-f^k)\n",
    "    diff_accumulated = []\n",
    "    for idx, diff in enumerate(stacked_diff):\n",
    "        if idx==0:\n",
    "            diff_accumulated.append(stacked_diff[idx])\n",
    "        else:\n",
    "            diff_accumulated.append(diff_accumulated[idx-1] + stacked_diff[idx])\n",
    "    diff_accumulated = np.stack(diff_accumulated)\n",
    "    # mean over the test set\n",
    "    diff_accumulated_mean = np.mean(diff_accumulated, axis=-1)\n",
    "    # divide by 1/L+1\n",
    "    diff_accumulated_mean_norm = np.array([])\n",
    "    for ix, d in enumerate(diff_accumulated_mean):\n",
    "        diff_accumulated_mean_norm = np.append(diff_accumulated_mean_norm, d/(ix+1))\n",
    "    # insert (0,0)\n",
    "    diff_accumulated_mean_norm_started_from_0 = np.insert(diff_accumulated_mean_norm,0,0.0)\n",
    "    # plot line\n",
    "    if plot:\n",
    "        x = np.arange(len(diff_accumulated_mean_norm_started_from_0))\n",
    "        plt.xlabel(\"permutation steps\")\n",
    "        plt.ylabel(\"AOPC\")\n",
    "        plt.plot(x, diff_accumulated_mean_norm_started_from_0, color =\"red\")\n",
    "        #plt.show()\n",
    "        plt.savefig(os.path.join(\"results\",model_name, \"{}-aopc-{}-{}.svg\".format(dataset_name, method, str(\"resnet_all\"))))\n",
    "    return diff_accumulated_mean_norm_started_from_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pixel_permutated_perturb_reverse = calculate_aopc(channel_ranking_pixel_permutation, method='pixel-permutation-perturb', perturb=True, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation by methods from captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Occlusion, DeepLift, IntegratedGradients, LRP\n",
    "from time import process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time()\n",
    "ablator = Occlusion(model)\n",
    "dataset = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                           label_map=label_map,\n",
    "                           selected_channels=np.arange(12),\n",
    "                           transform=transforms.Compose(test_transform),\n",
    "                           scaling_factor=scaling_factor)\n",
    "dataloader = DataLoader(dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "\n",
    "heatmaps = torch.empty(0, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data[0].to(device).float(), data[1].to(device).reshape(-1).long()\n",
    "        attr = ablator.attribute(inputs, target=labels, sliding_window_shapes=(1,3,3))\n",
    "        heatmaps = torch.cat((heatmaps, torch.from_numpy(np.percentile(torch.flatten(attr, start_dim=-2).cpu().numpy(), q=50, axis=-1)).to(dev)))\n",
    "heatmaps_mean = torch.mean(heatmaps, dim=0)\n",
    "plt.bar(channels[np.asarray(only_channels)], heatmaps_mean.cpu(), color='grey')\n",
    "plt.savefig(os.path.join(\"results\",model_name, \"{}-occl_method-model-50-percentile-{}.png\".format(dataset_name, str(\"resnet_all\"))))\n",
    "\n",
    "t1_stop = process_time()\n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "   \n",
    "print(\"Elapsed time during the whole program in seconds:\",\n",
    "                                         t1_stop-t1_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_occlusion = pd.DataFrame(data={'channels': channels[np.asarray(only_channels)], 'importance': heatmaps_mean.cpu().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pixel_ocll_perturb_reverse = calculate_aopc(channel_ranking_occlusion, method='pixel-occlusion-perturb-reverse', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time()\n",
    "ablator = DeepLift(model)\n",
    "\n",
    "dataset = DatasetGenerator(metadata=metadata.loc[test_index,:],\n",
    "                           label_map=label_map,\n",
    "                           selected_channels=np.arange(12),\n",
    "                           transform=transforms.Compose(test_transform),\n",
    "                           scaling_factor=scaling_factor)\n",
    "testloader = DataLoader(test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "\n",
    "heatmaps_deeplift = torch.empty(0, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device).float(), data[1].to(device).reshape(-1).long()\n",
    "        # baselines=torch.zeros(inputs.shape).to(dev)\n",
    "        attr = ablator.attribute(inputs, target=labels)\n",
    "        heatmaps_deeplift = torch.cat((heatmaps_deeplift,  torch.from_numpy(np.percentile(torch.flatten(attr, start_dim=-2).cpu().numpy(), q=50, axis=-1)).to(device)))\n",
    "heatmaps_deeplift_mean = torch.mean(heatmaps_deeplift, dim=0)\n",
    "plt.bar(channels, heatmaps_deeplift_mean.cpu(), color='grey')\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-deeplift_method-model-50-percentile-{}.png\".format(dataset_name, str(\"resnet_all\"))))\n",
    "\n",
    "t1_stop = process_time()\n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "   \n",
    "print(\"Elapsed time during the whole program in seconds:\",\n",
    "                                         t1_stop-t1_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_deep_lift = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': heatmaps_deeplift_mean.cpu().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_deep_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_deep_lift_perturb_reverse = calculate_aopc(channel_ranking_deep_lift, method='deep-lift-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_random = pd.DataFrame(data={'channels': channels[selected_channels], 'importance': np.random.randint(12, size=12)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ranking_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_random_perturb_reverse = calculate_aopc(channel_ranking_random, method='random-perturb-aopc', ascending=False, perturb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(res_random_perturb_reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 13})\n",
    "got_label=False\n",
    "plt.plot(x, res_deep_lift_perturb_reverse, label  = \"Channel-wise DeepLift\", color=\"orange\")\n",
    "plt.plot(x, res_pixel_ocll_perturb_reverse, label  = \"Channel-wise Occlusion\", color=\"green\")\n",
    "plt.plot(x, res_pixel_permutated_perturb_reverse, label  = \"Pixel-Permutation\", color=\"red\")\n",
    "plt.plot(x, res_random_perturb_reverse, label  = \"Random Baseline\", color=\"blue\")\n",
    "plt.xlabel('Perturbation steps')\n",
    "plt.ylabel('AOPC')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-{}.svg\".format(dataset_name, str(\"resnet_all\"))))\n",
    "#plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the random channel ranking 100 times to estimate the lower und upper bound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=2.576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_border = mean + (z * (std / np.sqrt(len(random_rankings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_border = mean - (z * (std / np.sqrt(len(random_rankings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    channel_ranking_random = pd.DataFrame(data={'channels': channels[np.asarray(only_channels)], 'importance': np.random.randint(12, size=12)})\n",
    "    random_rankings.append(calculate_aopc(channel_ranking_random, method='random-perturb-reverse', ascending=False, perturb=True, plot=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 13})\n",
    "got_label=False\n",
    "for ranking in random_rankings:\n",
    "    if not got_label:\n",
    "        plt.plot(x, ranking, label  = \"Random Baseline\", color=\"grey\", linewidth=0.5, alpha=0.1)\n",
    "        got_label=True\n",
    "    else:\n",
    "        plt.plot(x, ranking, color=\"grey\", linewidth=0.5, alpha=0.1)\n",
    "plt.plot(x, res_deep_lift_perturb_reverse, label  = \"Channel-wise DeepLift\", color=\"orange\")\n",
    "plt.plot(x, res_pixel_ocll_perturb_reverse, label  = \"Channel-wise Occlusion\", color=\"green\")\n",
    "plt.plot(x, res_pixel_permutated_perturb_reverse, label  = \"Pixel-Permutation\", color=\"red\")\n",
    "#plt.plot(x, res_random_perturb_reverse, label  = \"Random Baseline\", color=\"blue\")\n",
    "plt.xlabel('Perturbation steps')\n",
    "plt.ylabel('AOPC')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-1010-{}.svg\".format(dataset_name, str(\"resnet_all\"))))\n",
    "plt.savefig(os.path.join(\"results\", \"resnet_all\", \"{}-aopc-all-methods-1010-{}.png\".format(dataset_name, str(\"resnet_all\"))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newiflai",
   "language": "python",
   "name": "newiflai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}